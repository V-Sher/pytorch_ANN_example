{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import chain\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.nn import Module, Sigmoid, Linear, ReLU, Softmax\n",
    "\n",
    "from torch.utils.data import Dataset, random_split, DataLoader\n",
    "from torch.optim import SGD\n",
    "from torch.nn.init import kaiming_uniform_, xavier_uniform_\n",
    "from torch.nn import CrossEntropyLoss, BCELoss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>area</th>\n",
       "      <th>application_outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "      <td>rural</td>\n",
       "      <td>approved</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>42</td>\n",
       "      <td>rural</td>\n",
       "      <td>approved</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>57</td>\n",
       "      <td>urban</td>\n",
       "      <td>approved</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>32</td>\n",
       "      <td>rural</td>\n",
       "      <td>declined</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>29</td>\n",
       "      <td>urban</td>\n",
       "      <td>declined</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age   area application_outcome\n",
       "0   61  rural            approved\n",
       "1   42  rural            approved\n",
       "2   57  urban            approved\n",
       "3   32  rural            declined\n",
       "4   29  urban            declined"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the dataset\n",
    "filepath = '/Users/root947/Desktop/Oodle/df_cleaned.csv'\n",
    "df = pd.read_csv(filepath)[['age', 'area', 'application_outcome']]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the data - making it ALL numerical\n",
    "\n",
    "#convert area column to numerical\n",
    "le = LabelEncoder()\n",
    "df['area'] = le.fit_transform(df.area.values)\n",
    "    \n",
    "# scale the age column to between 0 and 1\n",
    "scaler = MinMaxScaler(feature_range = (0,1))\n",
    "age_t = scaler.fit_transform(df.age.values.reshape(-1, 1))\n",
    "df['age'] = list(chain(*(age_t)))\n",
    "    \n",
    "#converting the outcome variable to binary\n",
    "df['application_outcome'] = le.fit_transform(df.application_outcome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset definition - using Dataset\n",
    "\n",
    "class CSVDataset(Dataset):\n",
    "    \n",
    "    def __init__(self):\n",
    "    \n",
    "        #store the input and output features\n",
    "        self.X = df.values[:,:-1] # all rows + all columns except last\n",
    "        self.y = df.values[:,-1] # all rows + last column only\n",
    "    \n",
    "        # ensure all data is numerical - type(float)\n",
    "        self.X = self.X.astype('float32')\n",
    "        self.y = self.y.astype('float32')\n",
    "        #self.y = self.y.reshape((len(self.y), 1))\n",
    "    \n",
    "    # number of rows in dataset\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    # get a row at an index\n",
    "    def __getitem__(self, index):\n",
    "        return [self.X[index], self.y[index]]\n",
    "    \n",
    "    # split into train and testset - using `random_split`\n",
    "    def get_splits(self, split_ratio = 0.2):\n",
    "        test_size = round(split_ratio * len(self.X))\n",
    "        train_size = len(self.X) - test_size\n",
    "        \n",
    "        return random_split(self, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definition - using Module\n",
    "\n",
    "class myMLPNetwork(Module):\n",
    "    \n",
    "    # initialize the class\n",
    "    def __init__(self, n_inputs):\n",
    "        \n",
    "        # calling constructor of parent class\n",
    "        super().__init__()\n",
    "        \n",
    "        # defining the inputs to the first hidden layer - type of hidden layer, weights, activation\n",
    "        self.hid1 = Linear(n_inputs, 8) # equivalent to keras's dense layer\n",
    "        kaiming_uniform_(self.hid1.weight, nonlinearity='relu') # init the weights; Common examples include the Xavier and He weight initialization schemes\n",
    "        self.act1 = ReLU()\n",
    "        \n",
    "        # defining the inputs to the second hidden layer\n",
    "        self.hid2 = Linear(8, 16)\n",
    "        kaiming_uniform_(self.hid2.weight, nonlinearity='relu')\n",
    "        self.act2 = ReLU()\n",
    "        \n",
    "        # defining the inputs to the third hidden layer\n",
    "        self.hid3 = Linear(16, 2)\n",
    "        xavier_uniform_(self.hid3.weight)\n",
    "        self.act3 = Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \n",
    "        #input and act for layer 1\n",
    "        X = self.hid1(X)\n",
    "        X = self.act1(X)\n",
    "        \n",
    "        #input and act for layer 2\n",
    "        X = self.hid2(X)\n",
    "        X = self.act2(X)\n",
    "        \n",
    "        #input and act for layer 3\n",
    "        X = self.hid3(X)\n",
    "        X = self.act3(X)\n",
    "        \n",
    "        return X\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the dataset before training the model\n",
    "\n",
    "# load the dataset\n",
    "dataset = CSVDataset()\n",
    "\n",
    "# get the train and test split\n",
    "train, test = dataset.get_splits()\n",
    "\n",
    "# prepare dataloaders - essentially create batches (for both train and test) to be sent as input to the model\n",
    "train_dl = DataLoader(train, batch_size = 32, shuffle = True)\n",
    "test_dl = DataLoader(test, batch_size= 32, shuffle= False) # because we need unshuffled labels at the end to draw confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "\n",
    "# define the network\n",
    "model = myMLPNetwork(2) # 2 because we only have 2 input features\n",
    "\n",
    "# define the number of epochs\n",
    "epochs = 10\n",
    "\n",
    "# define the optimizer -SGD\n",
    "optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "# define the loss function\n",
    "criterion = CrossEntropyLoss()\n",
    "\n",
    "# iterate through all the epoch\n",
    "for epoch in range(epochs):\n",
    "    # go through all the batches generated by dataloader\n",
    "    for i, (inputs, targets) in enumerate(train_dl):\n",
    "            # clear the gradients\n",
    "            optimizer.zero_grad()\n",
    "            # compute the model output\n",
    "            yhat = model(inputs)\n",
    "            # calculate loss\n",
    "            loss = criterion(yhat, targets.type(torch.LongTensor))\n",
    "            # credit assignment\n",
    "            loss.backward()\n",
    "            # update model weights\n",
    "            optimizer.step()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "predictions, actuals = list(), list()\n",
    "\n",
    "# loop over all batches in test set\n",
    "\n",
    "for i, (inputs, targets) in enumerate(test_dl):\n",
    "    # pass input to the model\n",
    "    y_pred = model(inputs) \n",
    "    # retrieve the numpy array\n",
    "    y_pred = y_pred.detach().numpy()\n",
    "    # pick the index of the highest values\n",
    "    res = np.argmax(y_pred, axis = 1) \n",
    "    \n",
    "    # actual output\n",
    "    actual = targets.numpy()\n",
    "    actual = actual.reshape(len(actual), 1)\n",
    "    \n",
    "    # store the values in respective lists\n",
    "    predictions.append(list(res))\n",
    "    actuals.append(list(actual))\n",
    "    \n",
    "actuals = [val for sublist in vstack(list(chain(*actuals))) for val in sublist]\n",
    "predictions = [val for sublist in vstack(list(chain(*predictions))) for val in sublist]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7495"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluating how good the mdoel is\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(actuals, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
